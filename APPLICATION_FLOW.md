# AI Orchestrator Studio - Application Flow & Architecture

## 1. HIGH-LEVEL ARCHITECTURE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (React 18)                          â”‚
â”‚                   Port 3000 (TypeScript)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ChatStudio Component                                      â”‚ â”‚
â”‚  â”‚  - Model Selection                                         â”‚ â”‚
â”‚  â”‚  - Routing Profile Selection                              â”‚ â”‚
â”‚  â”‚  - Conversation Management                                â”‚ â”‚
â”‚  â”‚  - Message Sending/Receiving                              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“ HTTP/REST
                         (axios, 120s timeout)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKEND (FastAPI)                            â”‚
â”‚                   Port 8000 (Python 3.11)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  API Layer (Routers)                                       â”‚ â”‚
â”‚  â”‚  - /api/chat/ui/* (UI operations)                         â”‚ â”‚
â”‚  â”‚  - /api/llm/* (LLM config)                               â”‚ â”‚
â”‚  â”‚  - /v1/chat (Chat orchestration)                         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Service Layer                                             â”‚ â”‚
â”‚  â”‚  - ChatRouter (routing logic)                             â”‚ â”‚
â”‚  â”‚  - OrchestrationGraph (state machine)                     â”‚ â”‚
â”‚  â”‚  - Clients (LLM, External Agent, DataSource)             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Infrastructure                                            â”‚ â”‚
â”‚  â”‚  - Database (Conversations, Messages, Profiles)           â”‚ â”‚
â”‚  â”‚  - Memory/Cache (ConversationMemory, CacheManager)       â”‚ â”‚
â”‚  â”‚  - Configuration (.env, Settings)                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“              â†“                    â†“              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   LLM   â”‚   â”‚ External â”‚        â”‚   Data   â”‚   â”‚  Tools   â”‚
    â”‚ Server  â”‚   â”‚  Agent   â”‚        â”‚  Source  â”‚   â”‚ Registry â”‚
    â”‚         â”‚   â”‚          â”‚        â”‚          â”‚   â”‚          â”‚
    â”‚ Ollama/ â”‚   â”‚ Custom   â”‚        â”‚Database  â”‚   â”‚External  â”‚
    â”‚OpenAI   â”‚   â”‚Orchestr. â”‚        â”‚APIs      â”‚   â”‚Services  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 1.a FLOW DIAGRAM (Mermaid)

```mermaid
graph TD
   %% UI Layer
   U[User Prompt] -->|HTTP/REST| UI[Frontend: ChatStudio]
   UI -->|POST /api/chat/ui/send (SSE)| BE[Backend: FastAPI]
   UI -->|GET /api/chat/ui/models| BE
   UI -->|GET /api/chat/ui/profiles| BE
   UI -->|GET /api/chat/ui/conversations| BE

   %% Orchestration Graph
   BE --> IR[Intent Router]
   IR -->|route| PL[Planner]
   PL -->|plan| LA[LLM Agent]
   LA -->|tool_call| TE[Tool Executor]
   TE -->|fetch_data| GR[Grounding]
   GR -->|context| LA
   LA -->|store| MS[Memory Store]
   MS -->|log| AU[Audit]
   AU --> END[End]

   %% Direct LLM path
   IR -->|direct_llm| LA

   %% External Agent path
   LA --> EA[External Agent]
   EA --> LA

   %% Data Sources and Tools
   TE -->|HTTP, Search, Code| DS[(Datasources)]
   GR --> DS

   %% Response back to UI
   END -->|SendMessageResponse + metadata| UI
```

This diagram reflects the implemented topology and endpoints, including SSE streaming to the UI, routing profiles, planning, tool execution, grounding, memory updates, and auditing.

---

## 2.a End-to-End Flow (Prompt â†’ Answer)

- Normalize Input: Clean text, resolve dates/timezone, create normalized fingerprint for caching.
- Rule-Based Router (no LLM): Decide Analytics vs Documents vs General Chat.
- Decide Complexity: Simple vs Complex; choose Cache vs Execution vs Planner; saves ~200â€“300 ms per request.
- Caching (before any LLM):
   - Exact match â†’ Redis.
   - Semantic similarity â†’ Qdrant/FAISS.
   - Reuse plan or result when possible.
- Analytics Path:
   - Load semantic contract (metrics, dimensions, rules).
   - Metric selection (rules + semantic similarity).
   - Planner LLM only if complex; deterministic plan for simple queries.
   - Query Validation Layer (hard gate): validate metric existence, dimensions, inject mandatory filters, block PII, enforce limits.
   - Template-Based Query Build: no free SQL; no hallucinated joins; deterministic execution format.
   - Result Cache: skip execution if cached.
   - Execute Data Source: only governed APIs; capture latency & metadata.
   - Result Validation (second gate): empty results, spikes, anomalies; retry once via fallback.
   - Grounded Answer: LLM only writes explanation using returned data; includes evidence & freshness.
   - Cache Writeback: plan cache, result cache, semantic similarity cache.
   - Observability: full latency breakdown; cache hit/miss; planner usage; LLM tokens; SLA tracking.

```mermaid
flowchart TD
   A[Prompt] --> N[Normalize Input\nClean text, resolve dates/timezone\nFingerprint]
   N --> R[Rule-Based Router\n(no LLM)]
   R -->|Analytics| AN
   R -->|Documents| DOC
   R -->|General Chat| GC

   %% Caching before LLM
   subgraph CACHING[Early Caching]
      CM[Exact Cache\nRedis]
      CS[Semantic Cache\nQdrant/FAISS]
   end
   N --> CACHING
   CACHING --> R

   %% Analytics path
   AN --> DCC[Decide: Simple vs Complex\nCache vs Execution vs Planner]
   DCC --> SC[Semantic Contract\nLoad metrics, dimensions, rules]
   SC --> MS[Metric Selection\nRules + similarity]
   MS --> PL{Complex?}
   PL -->|No| DP[Deterministic Plan]
   PL -->|Yes| PLLM[Planner LLM\n(JSON plan only)]
   DP --> QV
   PLLM --> QV[Query Validation (Hard Gate)\nValidate metrics & dims\nInject filters\nBlock PII\nEnforce limits]
   QV --> TB[Template-Based Query Build\nNo free SQL\nNo hallucinated joins]
   TB --> RC{Result Cached?}
   RC -->|Yes| GRNA[Grounded Answer\nLLM writes explanation\nUses returned data only]
   RC -->|No| EXE[Execute Data Source\nGoverned APIs]
   EXE --> RV[Result Validation (Second Gate)\nEmpty/spikes/anomalies\nRetry once]
   RV --> GRNA
   GRNA --> WB[Cache Writeback\nPlan/result/semantic]

   %% Observability
   subgraph OBS[Observability]
      LAT[Latency breakdown]
      HIT[Cache hit/miss]
      PUSE[Planner usage]
      TOK[LLM tokens]
      SLA[SLA tracking]
   end
   WB --> OBS

   %% Documents & General Chat (grounded)
   DOC --> DPROC[Retrieve + Ground]\n
   GC --> GPROC[Guardrails + Ground]\n
   DPROC --> GRNA
   GPROC --> GRNA
```

---

## 2.b Where Each Technology Lives (Clear Mapping)

- LangGraph: Control plane (state machine). Used for routing, planner decision, validation gates, retry/fallback logic, end-to-end orchestration. LangGraph owns the flow, not the intelligence.
- LangChain: Building blocks. Used for tool wrappers, vector retrievers, prompt templates, output parsers (JSON only). LangChain supplies components; it never decides the flow.
- LLM (e.g., Llama 4 Scout on H100): Used only for complex planning (JSON plan) and grounded answer writing. Never used for routing, execution, validation, SQL/joins.

---

## 2.c Caching Layers

| Cache Type | Technology       | Purpose                      |
|------------|------------------|------------------------------|
| Exact      | Redis            | Instant responses            |
| Semantic   | Qdrant / FAISS   | Handle 20%+ similar queries  |
| Result     | Redis            | Reduce backend load          |


## 2. REQUEST FLOW: USER MESSAGE â†’ RESPONSE

### 2.1 Frontend Flow (ChatStudio.tsx)

```
1. USER INTERACTION
   â”œâ”€ User selects Model (loadModels)
   â”‚  â””â”€ GET /api/chat/ui/models
   â”‚     â””â”€ Returns: {models: [{id, name}], default_model, success}
   â”‚
   â”œâ”€ User selects Routing Profile (loadRoutingProfiles)
   â”‚  â””â”€ GET /api/chat/ui/profiles
   â”‚     â””â”€ Returns: {profiles: [{id, name, description}]}
   â”‚
   â””â”€ User loads conversation history (loadConversationMessages)
      â””â”€ GET /api/chat/ui/conversations/{conversationId}
         â””â”€ Returns: {conversation, messages: [{id, role, content, metadata}]}

2. MESSAGE SENDING
   â”œâ”€ ChatInput.tsx captures user text
   â”‚  â””â”€ onSend callback triggered
   â”‚
   â””â”€ handleSendMessage() sends via axios
      â”œâ”€ POST /api/chat/ui/send
      â”‚  â”œâ”€ Payload:
      â”‚  â”‚  {
      â”‚  â”‚    conversation_id: string,
      â”‚  â”‚    message: string,
      â”‚    model_id: string,
      â”‚  â”‚    routing_profile: "direct_llm" | "zain_agent" | "tools_data",
      â”‚  â”‚    use_memory: boolean,
      â”‚  â”‚    use_tools: boolean,
      â”‚  â”‚    metadata: object
      â”‚  â”‚  }
      â”‚  â”‚
      â”‚  â”œâ”€ Timeout: 120 seconds (for slow external LLM servers)
      â”‚  â””â”€ Returns: {conversation_id, message_id, answer, metadata, error?}
      â”‚
      â””â”€ On response:
         â”œâ”€ setMessages() updates local state
         â”œâ”€ loadConversationMessages() refreshes from backend
         â””â”€ scrollToBottom() shows latest message

3. ERROR HANDLING
   â”œâ”€ Timeout (120s): "Connection to LLM server timed out"
   â”œâ”€ Connection refused: "Failed to connect to backend server"
   â””â”€ LLM config missing: "No LLM models available. Go to Settings > LLM Configuration"
```

### 2.2 Backend Flow: /api/chat/ui/send

```
ENTRY POINT: chat_ui.send_message()
â”œâ”€ 1. CONVERSATION MANAGEMENT
â”‚  â”œâ”€ If conversation_id provided:
â”‚  â”‚  â””â”€ Query database for existing conversation
â”‚  â”‚
â”‚  â””â”€ Else (new conversation):
â”‚     â””â”€ Create new Conversation record:
â”‚        {
â”‚          title: "Chat {timestamp}",
â”‚          model_id: from request,
â”‚          routing_profile: from request,
â”‚          user_id: "default",
â”‚          created_at: now()
â”‚        }
â”‚        â””â”€ Store in database
â”‚
â”œâ”€ 2. MESSAGE PERSISTENCE
â”‚  â”œâ”€ Create user Message record:
â”‚  â”‚  {conversation_id, role="user", content, metadata}
â”‚  â”‚  â””â”€ Store in database
â”‚  â”‚
â”‚  â””â”€ (Assistant message stored AFTER routing completes)
â”‚
â”œâ”€ 3. MESSAGE ROUTING
â”‚  â””â”€ ChatRouter.route_message()
â”‚     â”œâ”€ Intent Classification (optional)
â”‚     â”‚  â””â”€ classify_intent(text) â†’ returns intent + confidence
â”‚     â”‚
â”‚     â”œâ”€ Route Based on Routing Profile:
â”‚     â”‚  â”œâ”€ "direct_llm":
â”‚     â”‚  â”‚  â””â”€ Direct LLMClient.generate_response()
â”‚     â”‚  â”‚
â”‚     â”‚  â”œâ”€ "zain_agent":
â”‚     â”‚  â”‚  â”œâ”€ Optional: Call external agent
â”‚     â”‚  â”‚  â””â”€ Get response from LLM
â”‚     â”‚  â”‚
â”‚     â”‚  â””â”€ "tools_data":
â”‚     â”‚     â”œâ”€ Plan tools and data sources
â”‚     â”‚     â”œâ”€ Execute tools and fetch data
â”‚     â”‚     â””â”€ Synthesize response via LLM
â”‚     â”‚
â”‚     â””â”€ Returns: {answer, metadata: {tokens, execution_steps, tools_used, model}}
â”‚
â”œâ”€ 4. RESPONSE STORAGE
â”‚  â””â”€ Create assistant Message record:
â”‚     {conversation_id, role="assistant", content=answer, metadata}
â”‚     â””â”€ Store in database
â”‚
â”œâ”€ 5. RESPONSE RETURN
â”‚  â””â”€ SendMessageResponse:
â”‚     {
â”‚       conversation_id,
â”‚       message_id,
â”‚       answer,
â”‚       metadata: {
â”‚         tokens?: number,
â”‚         model: string,
â”‚         tools_used?: [{name, input, output, duration_ms}],
â”‚         execution_steps?: [{step, timestamp, status}]
â”‚       },
â”‚       error?: string
â”‚     }
â”‚
â””â”€ 6. EXCEPTION HANDLING
   â””â”€ If error occurs:
      â””â”€ HTTPException(status_code=500, detail=error message)
```

---

## 3. LLM CONFIGURATION FLOW

### 3.1 Frontend: Settings â†’ LLM Configuration

```
User navigates to Settings > LLM Configuration
â”‚
â”œâ”€ LOAD CURRENT CONFIG
â”‚  â””â”€ GET /api/llm/config
â”‚     â””â”€ Returns: {base_url, default_model, api_key?, timeout_seconds, temperature, max_tokens}
â”‚
â”œâ”€ UPDATE CONFIG
â”‚  â””â”€ PUT /api/llm/config
â”‚     â”œâ”€ Payload:
â”‚     â”‚  {
â”‚     â”‚    base_url: "http://10.99.70.200:4000",
â”‚     â”‚    default_model: "llama4-scout",
â”‚     â”‚    api_key?: string,
â”‚     â”‚    timeout_seconds: 120,
â”‚     â”‚    temperature: 0.7,
â”‚     â”‚    max_tokens?: number
â”‚     â”‚  }
â”‚     â”‚
â”‚     â””â”€ Timeout: Default fetch timeout (not 120s like chat)
â”‚
â”œâ”€ TEST CONNECTION
â”‚  â””â”€ POST /api/llm/test
â”‚     â”œâ”€ Payload: {prompt?, model?}
â”‚     â””â”€ Returns: {success, message, response?, error?, response_time_ms}
â”‚
â””â”€ ON SUCCESS
   â””â”€ Redirect to ChatStudio (models now loaded)
```

### 3.2 Backend: LLM Configuration Update & Validation

```
ENTRY POINT: llm.update_llm_config()
â”‚
â”œâ”€ 1. URL VALIDATION & NORMALIZATION
â”‚  â”œâ”€ Parse base_url with urllib.parse.urlsplit()
â”‚  â”œâ”€ Validate: Must have scheme (http/https) and netloc (host:port)
â”‚  â”œâ”€ Normalize: Strip trailing paths like /api/chat
â”‚  â”‚  â””â”€ Keep only: scheme + netloc (e.g., "http://10.99.70.200:4000")
â”‚  â”‚
â”‚  â””â”€ If invalid format:
â”‚     â””â”€ HTTPException(400, "Base URL must include scheme and host, e.g., http://10.99.70.200:4000")
â”‚
â”œâ”€ 2. CONNECTIVITY PROBING (async probe_models)
â”‚  â”‚
â”‚  â”œâ”€ Primary URL Test:
â”‚  â”‚  â”œâ”€ Try: GET {base_url}/api/tags (Ollama API)
â”‚  â”‚  â”‚  â””â”€ If success: Parse models array
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ If fails, try: GET {base_url}/v1/models (OpenAI-compatible API)
â”‚  â”‚     â””â”€ If success: Parse models array
â”‚  â”‚
â”‚  â”œâ”€ If primary URL fails, try FALLBACK PORTS:
â”‚  â”‚  â”œâ”€ Port 9000
â”‚  â”‚  â”œâ”€ Port 4000 (already tried)
â”‚  â”‚  â”œâ”€ Port 8000
â”‚  â”‚  â””â”€ Port 3000
â”‚  â”‚  â””â”€ For each: Repeat /api/tags then /v1/models probes
â”‚  â”‚
â”‚  â”œâ”€ Timeout: Uses settings.llm_timeout_seconds (120s in current .env)
â”‚  â”‚
â”‚  â””â”€ Returns:
â”‚     â”œâ”€ If found: (True, models_list, normalized_url)
â”‚     â”‚  â””â”€ Example: ([{name: "llama4-scout"}, {name: "llama2"}], "http://10.99.70.200:4000")
â”‚     â”‚
â”‚     â””â”€ If not found: (False, None, error_message)
â”‚        â””â”€ Example: (False, None, "Failed to connect to http://10.99.70.200:4000 on any port")
â”‚
â”œâ”€ 3. MODEL AUTO-SELECTION (if no default specified)
â”‚  â””â”€ If probe found models but no default_model in request:
â”‚     â”œâ”€ auto_select_model = models[0]["name"]
â”‚     â””â”€ Use first discovered model as default
â”‚
â”œâ”€ 4. PERSISTENCE TO .env
â”‚  â”œâ”€ Write to backend/orchestrator/.env:
â”‚  â”‚  â”œâ”€ LLM_BASE_URL={normalized_url}
â”‚  â”‚  â”œâ”€ LLM_DEFAULT_MODEL={model_name}
â”‚  â”‚  â”œâ”€ LLM_TIMEOUT_SECONDS=120
â”‚  â”‚  â””â”€ (Other settings preserved)
â”‚  â”‚
â”‚  â””â”€ This survives server restart
â”‚
â”œâ”€ 5. SETTINGS RELOAD
â”‚  â””â”€ clear_settings_cache()
â”‚     â””â”€ Force Settings.get() to reload from .env on next call
â”‚
â”œâ”€ 6. RESPONSE
â”‚  â””â”€ Return:
â”‚     {
â”‚       success: true,
â”‚       message: "LLM configuration updated successfully",
â”‚       config: {
â”‚         base_url: "http://10.99.70.200:4000",
â”‚         default_model: "llama4-scout",
â”‚         models_available: ["llama4-scout", "llama2"],
â”‚         timeout_seconds: 120
â”‚       }
â”‚     }
â”‚
â””â”€ 7. ERROR HANDLING
   â”œâ”€ If URL malformed:
   â”‚  â””â”€ HTTPException(400, "Base URL must include scheme and host...")
   â”‚
   â”œâ”€ If no connectivity to any port:
   â”‚  â””â”€ HTTPException(400, "Failed to connect to http://10.99.70.200 on any port (9000, 4000, 8000, 3000). Server offline?")
   â”‚
   â””â”€ If other error:
      â””â”€ HTTPException(500, error message)
```

---

## 4. ORCHESTRATION GRAPH EXECUTION

### 4.1 State Machine Flow

```
ENTRY: OrchestrationGraph.execute(state)
â”‚
â”œâ”€ NODE 1: INTENT CLASSIFICATION
â”‚  â”œâ”€ Input: user_input
â”‚  â”œâ”€ Process: classify_intent(text)
â”‚  â”‚  â””â”€ Uses LLM or pattern matching to classify intent
â”‚  â”‚     Examples: "query", "command", "data_request", "general"
â”‚  â”‚
â”‚  â””â”€ Output: state.intent, state.intent_confidence
â”‚
â”œâ”€ NODE 2: PLANNING (if required)
â”‚  â”œâ”€ Input: intent, user_input
â”‚  â”œâ”€ Process: Planner.create_plan(intent, input)
â”‚  â”‚  â””â”€ Creates TaskPlan with required tools/data sources
â”‚  â”‚
â”‚  â””â”€ Output: state.plan (list of tasks to execute)
â”‚
â”œâ”€ NODE 3: TOOL EXECUTION (if use_tools=True)
â”‚  â”œâ”€ Input: state.plan
â”‚  â”œâ”€ Process:
â”‚  â”‚  â”œâ”€ For each tool in plan:
â”‚  â”‚  â”‚  â”œâ”€ Lookup in ToolRegistry
â”‚  â”‚  â”‚  â”œâ”€ Execute tool with input parameters
â”‚  â”‚  â”‚  â””â”€ Capture result + duration
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Aggregate results for grounding
â”‚  â”‚
â”‚  â””â”€ Output: state.tool_results
â”‚
â”œâ”€ NODE 4: DATA GROUNDING (if datasource configured)
â”‚  â”œâ”€ Input: intent, user_input, tool_results
â”‚  â”œâ”€ Process: Fetch relevant data from data sources
â”‚  â”‚  â””â”€ Uses DataSourceClient to query configured datasources
â”‚  â”‚
â”‚  â””â”€ Output: state.grounding_data
â”‚
â”œâ”€ NODE 5: EXTERNAL AGENT (if configured and routed)
â”‚  â”œâ”€ Input: user_input, tool_results, grounding_data
â”‚  â”œâ”€ Process: Send to external agent if routing_profile="zain_agent"
â”‚  â”‚  â””â”€ ExternalAgentClient.call_agent()
â”‚  â”‚
â”‚  â””â”€ Output: state.external_agent_result
â”‚
â”œâ”€ NODE 6: LLM GENERATION
â”‚  â”œâ”€ Input: user_input, intent, tool_results, grounding_data, agent_result
â”‚  â”œâ”€ Process: LLMClient.generate_response()
â”‚  â”‚  â”œâ”€ Build context from all previous nodes
â”‚  â”‚  â”œâ”€ Call LLM server (Ollama or OpenAI-compatible)
â”‚  â”‚  â”œâ”€ Timeout: 120 seconds (settings.llm_timeout_seconds)
â”‚  â”‚  â””â”€ Capture tokens used, generation time
â”‚  â”‚
â”‚  â””â”€ Output: state.llm_response, state.final_metadata
â”‚
â”œâ”€ NODE 7: GROUNDING & SYNTHESIS (optional)
â”‚  â”œâ”€ Input: llm_response, grounding_data
â”‚  â”œâ”€ Process: Post-process response
â”‚  â”‚  â””â”€ Inject citations, verify facts against grounding data
â”‚  â”‚
â”‚  â””â”€ Output: state.answer
â”‚
â”œâ”€ NODE 8: MEMORY UPDATE (if use_memory=True)
â”‚  â”œâ”€ Input: user_input, answer, metadata
â”‚  â”œâ”€ Process: ConversationMemory.add_exchange()
â”‚  â”‚  â”œâ”€ Store in local memory (for context in next message)
â”‚  â”‚  â””â”€ Optional: Store in Redis cache
â”‚  â”‚
â”‚  â””â”€ Output: Memory updated for next iteration
â”‚
â””â”€ FINALIZE
   â”œâ”€ Cleanup resources (close clients, release connections)
   â”œâ”€ Return final_state with:
   â”‚  â”œâ”€ answer: final response text
   â”‚  â”œâ”€ final_metadata: {tokens, execution_time, model, tools_used, execution_steps}
   â”‚  â”œâ”€ execution_id: tracking ID
   â”‚  â””â”€ error: null (or error message if failed)
   â”‚
   â””â”€ Backend returns to chat_ui.send_message()
```

---

## 5. DATA FLOW: CONVERSATION PERSISTENCE

```
Database Schema:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conversation                                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id: UUID (primary key)                                      â”‚
â”‚ title: str                                                  â”‚
â”‚ model_id: str                                               â”‚
â”‚ routing_profile: str ("direct_llm", "zain_agent", ...)    â”‚
â”‚ user_id: str                                                â”‚
â”‚ created_at: datetime                                        â”‚
â”‚ updated_at: datetime                                        â”‚
â”‚ is_deleted: bool (soft delete)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“ 1:Many
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Message                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id: UUID (primary key)                                      â”‚
â”‚ conversation_id: UUID (foreign key â†’ Conversation)         â”‚
â”‚ role: str ("user" or "assistant")                          â”‚
â”‚ content: text                                               â”‚
â”‚ metadata: JSON {                                            â”‚
â”‚   tokens?: number,                                          â”‚
â”‚   model: string,                                            â”‚
â”‚   tools_used?: [{name, duration_ms}],                      â”‚
â”‚   execution_steps?: [{step, status}]                       â”‚
â”‚ }                                                           â”‚
â”‚ created_at: datetime                                        â”‚
â”‚ is_deleted: bool (soft delete)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Read Flow:
1. Frontend calls: GET /api/chat/ui/conversations/{id}
2. Backend queries: SELECT * FROM Message WHERE conversation_id=?
3. Order by: created_at ASC
4. Return: {conversation, messages: [{id, role, content, metadata, created_at}]}
5. Frontend renders: ChatMessage components for each message

Write Flow:
1. Frontend sends: POST /api/chat/ui/send {...message...}
2. Backend creates: Conversation record (if new)
3. Backend creates: User Message record
4. Backend executes: Orchestration Graph
5. Backend creates: Assistant Message record
6. Backend returns: SendMessageResponse
7. Frontend queries: GET /api/chat/ui/conversations/{id}
8. Frontend reloads messages and renders
```

---

## 6. CONFIGURATION MANAGEMENT

```
.env File (backend/orchestrator/.env)
â”œâ”€ API Configuration
â”‚  â”œâ”€ API_PORT=8000
â”‚  â”œâ”€ DEBUG=false
â”‚  â””â”€ APP_NAME=ZainOne Orchestrator
â”‚
â”œâ”€ LLM Configuration
â”‚  â”œâ”€ LLM_BASE_URL=http://10.99.70.200:4000
â”‚  â”œâ”€ LLM_DEFAULT_MODEL=llama4-scout
â”‚  â”œâ”€ LLM_TIMEOUT_SECONDS=120 (CRITICAL: allows slow external servers)
â”‚  â”œâ”€ LLM_TEMPERATURE=0.7
â”‚  â”œâ”€ LLM_MAX_TOKENS=2048
â”‚  â””â”€ LLM_MAX_RETRIES=3
â”‚
â”œâ”€ External Services (optional)
â”‚  â”œâ”€ EXTERNAL_AGENT_URL=
â”‚  â”œâ”€ DATA_SOURCE_URL=
â”‚  â””â”€ REDIS_URL= (or use in-memory cache)
â”‚
â””â”€ Database
   â””â”€ DATABASE_URL=sqlite:///./orchestrator.db

Settings Load Priority:
1. Load from .env file (persisted by update_llm_config)
2. Apply defaults from pydantic Settings model
3. Can be overridden by environment variables
4. Cached in memory (cleared by clear_settings_cache())
```

---

## 7. KEY INTEGRATION POINTS & CONCERNS

### ğŸ”´ CRITICAL CONCERNS - FRONTEND/BACKEND MISMATCH

#### 1. **Timeout Misalignment**
```
CONCERN: Frontend and backend timeouts can be out of sync
â”œâ”€ Frontend: axios timeout 120 seconds (hardcoded in ChatStudio.tsx)
â”œâ”€ Backend: settings.llm_timeout_seconds from .env (can be 60 or 120)
â””â”€ RISK: 
   â”œâ”€ If backend timeout < frontend: Backend might timeout but frontend still waiting
   â”œâ”€ If frontend timeout < backend: Frontend shows error while backend still processing
   â”‚                                   (creates zombie processes/responses)

MITIGATION:
â”œâ”€ Document: Frontend and backend timeouts must match
â”œâ”€ Config: Make both configurable from same source
â””â”€ Ideal: Move timeout to .env and have frontend read it from GET /api/config
```

#### 2. **Error Message Inconsistency**
```
CONCERN: Frontend and backend return different error formats
â”œâ”€ Frontend sends:
â”‚  â””â”€ Simple JSON: {conversation_id, message, model_id, routing_profile, ...}
â”‚
â”œâ”€ Backend expects this format but may return:
â”‚  â”œâ”€ Option A: SendMessageResponse {success: true/false, answer, metadata, error}
â”‚  â”œâ”€ Option B: HTTPException {detail: "error message"}
â”‚  â””â”€ Option C: Raw 500 error with stack trace (if unhandled exception)
â”‚
â””â”€ RISK:
   â”œâ”€ Frontend may not recognize error fields properly
   â”œâ”€ Different error codes (400, 404, 500) need frontend handling
   â””â”€ Error messages may expose internal implementation details
```

#### 3. **LLM Configuration Persistence Issue**
```
CONCERN: Model list shown to frontend may be stale after config change
â”œâ”€ User updates LLM config (PUT /api/llm/config)
â”œâ”€ User returns to ChatStudio
â”œâ”€ Calls: GET /api/chat/ui/models
â”œâ”€ Backend executes: probe_models(base_url, timeout, api_key)
â”œâ”€ RISK: 
â”‚  â”œâ”€ If old LLM_BASE_URL still in .env, backend may probe wrong server
â”‚  â”œâ”€ update_llm_config() writes to .env but may not reload immediately
â”‚  â””â”€ clear_settings_cache() should clear it, but timing issues possible
â”‚
â””â”€ VERIFICATION:
   â””â”€ Does update_llm_config() properly invalidate settings cache before returning?
      âœ“ YES: clear_settings_cache() called in endpoint
```

#### 4. **Routing Profile Frontend/Backend Mismatch**
```
CONCERN: Frontend hardcodes routing profiles, backend may add more
â”œâ”€ Frontend has: ["direct_llm", "zain_agent", "tools_data"]
â”‚  â””â”€ Fallback if API fails: hardcoded list
â”‚
â”œâ”€ Backend: GET /api/chat/ui/profiles returns from database
â”‚  â””â”€ If database empty: returns what?
â”‚     â”œâ”€ NOT explicitly defined in code
â”‚     â””â”€ Likely returns empty list, causing frontend fallback
â”‚
â””â”€ RISK:
   â”œâ”€ New routing profiles added to backend won't show in frontend
   â”œâ”€ Frontend might send profile that backend doesn't understand
   â””â”€ No validation on backend for unknown profiles
```

#### 5. **Model ID vs Model Name**
```
CONCERN: Inconsistent use of model identifiers
â”œâ”€ Frontend stores: Model.id, Model.name
â”œâ”€ Backend expects in send_message: model_id
â”œâ”€ LLM response returns: model name
â”‚
â””â”€ RISK:
   â”œâ”€ Model ID format varies: "llama4-scout" vs "gpt-4" vs full path
   â”œâ”€ Database Conversation.model_id stores model_id
   â”œâ”€ Message metadata stores model name
   â””â”€ QUESTION: Are they the same value?
      â””â”€ UNCLEAR from code
```

#### 6. **Message Streaming Not Implemented**
```
CONCERN: UI shows "loading" spinner, but no streaming response
â”œâ”€ Frontend: ChatStudio.tsx has "stream: false" hardcoded
â”œâ”€ Backend: /api/chat/ui/send has "stream" parameter
â”‚  â””â”€ Returns: full response, not streamed
â”‚
â””â”€ RISK:
   â”œâ”€ Long responses feel unresponsive (full 120s wait)
   â”œâ”€ User doesn't see tokens being generated in real-time
   â”œâ”€ Network timeout risk if response > timeout window
   â”‚
   â””â”€ RECOMMENDATION: Implement streaming
      â”œâ”€ Backend: StreamingResponse with Server-Sent Events
      â”œâ”€ Frontend: Read stream and update incrementally
      â””â”€ Better UX: user sees partial response as it generates
```

#### 7. **Memory Management Transparency**
```
CONCERN: Frontend doesn't know what's in conversation memory
â”œâ”€ Frontend sends: use_memory=true/false
â”œâ”€ Backend stores in: ConversationMemory (memory.py)
â”‚  â””â”€ Stored where? Memory class, Redis, or database?
â”‚
â”œâ”€ RISK:
â”‚  â”œâ”€ No endpoint to inspect memory contents
â”‚  â”œâ”€ User can't clear memory between conversations
â”‚  â”œâ”€ Memory accumulates tokens and costs money
â”‚  â”‚
â”‚  â””â”€ CONCERN: Does conversation memory carry across conversations?
â”‚     â”œâ”€ If yes: models may get confused between conversation contexts
â”‚     â””â”€ If no: use_memory flag is useless for multi-turn
```

#### 8. **Tool Execution Visibility**
```
CONCERN: Frontend shows tools_used in metadata but limited insight
â”œâ”€ Frontend displays: tool name, duration_ms
â”œâ”€ MISSING: tool input parameters, tool output, which step failed
â”‚
â””â”€ RISK:
   â”œâ”€ User can't debug why tool gave wrong output
   â”œâ”€ No way to verify tool was called with correct input
   â””â”€ Debug mode doesn't actually debug tools
```

### ğŸŸ¡ MODERATE CONCERNS

#### 9. **Database Transaction Safety**
```
CONCERN: Race condition in new conversation creation
â”œâ”€ POST /api/chat/ui/send:
â”‚  1. Create Conversation
â”‚  2. db.commit()
â”‚  3. Create User Message
â”‚  4. Create Assistant Message
â”‚  5. db.commit()
â”‚
â””â”€ RISK:
   â”œâ”€ If router fails between steps 2-3: conversation exists with no messages
   â”œâ”€ If router fails between steps 3-4: user message exists but no response
   â”œâ”€ No transaction wrapper to rollback on failure
```

#### 10. **Conversation Soft Delete Logic**
```
CONCERN: is_deleted flag but frontend doesn't properly respect it
â”œâ”€ Database: Conversation.is_deleted = True (soft delete)
â”œâ”€ Frontend: Calls DELETE /api/chat/ui/conversations/{id}
â”œâ”€ Backend: Sets is_deleted = True
â”‚
â””â”€ RISK:
   â”œâ”€ loadConversations() shows all conversations?
   â”‚  â””â”€ OR filters is_deleted=False?
   â”‚  â””â”€ CODE REVIEW NEEDED: Does chat_ui.list_conversations filter soft deletes?
   â”‚
   â””â”€ Potential: Deleted conversations still appear in sidebar
```

#### 11. **Metadata Field Inconsistency**
```
CONCERN: metadata structure varies across API responses
â”œâ”€ Message.metadata: {tokens?, model, tools_used?, execution_steps?}
â”œâ”€ SendMessageResponse.metadata: {tokens?, model, ...}
â”œâ”€ ChatRequest.metadata: arbitrary dict {custom fields}
â”‚
â””â”€ RISK:
   â”œâ”€ Frontend may expect fields that aren't always present
   â”œâ”€ Backend may send fields frontend doesn't use
   â”œâ”€ No schema validation for metadata content
```

### ğŸŸ¢ MINOR CONCERNS / RECOMMENDATIONS

#### 12. **HTTP Method Consistency**
```
OBSERVATION: Routes follow REST conventions
â”œâ”€ GET /conversations - list
â”œâ”€ POST /conversations - create
â”œâ”€ GET /conversations/{id} - read
â”œâ”€ DELETE /conversations/{id} - delete
â”œâ”€ POST /send - action
â””â”€ âœ“ GOOD: Follows REST conventions
```

#### 13. **API Versioning**
```
OBSERVATION: /v1/ prefix on /v1/chat endpoint but not others
â”œâ”€ /v1/chat (versioned - core API)
â”œâ”€ /api/chat/ui/* (unversioned - UI layer)
â”œâ”€ /api/llm/* (unversioned - configuration)
â”‚
â””â”€ âœ“ ACCEPTABLE: Different paths for different purposes
   â”œâ”€ /v1/ = public API (versioned)
   â””â”€ /api/ = internal UI APIs (unversioned)
```

---

## 8. FLOW SEQUENCE DIAGRAM: COMPLETE MESSAGE FLOW

```
Frontend                    Backend                    LLM Server
   â”‚                           â”‚                           â”‚
   â”‚  1. Load Models            â”‚                           â”‚
   â”œâ”€â”€GET /api/chat/ui/modelsâ”€â”€>â”‚                           â”‚
   â”‚                           â”‚  2. Probe models          â”‚
   â”‚                           â”œâ”€â”€â”€â”€â”€â”€GET /api/tagsâ”€â”€â”€â”€â”€â”€â”€>â”‚
   â”‚                           â”‚<â”€â”€â”€â”€â”€models listâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚                           â”‚                           â”‚
   â”‚<â”€â”€{models, default}â”€â”€â”€â”€â”€â”€â”€â”€â”¤                           â”‚
   â”‚                           â”‚                           â”‚
   â”‚  3. User types message      â”‚                           â”‚
   â”‚  4. Send message            â”‚                           â”‚
   â”œâ”€POST /api/chat/ui/sendâ”€â”€â”€â”€â”€>â”‚                           â”‚
   â”‚    {conversation_id,       â”‚  5. Create/Update Conv   â”‚
   â”‚     message,               â”‚  6. Store user message   â”‚
   â”‚     model_id,              â”‚                           â”‚
   â”‚     routing_profile,       â”‚  7. Route message        â”‚
   â”‚     use_memory,            â”‚  8. Build context        â”‚
   â”‚     use_tools}             â”‚                           â”‚
   â”‚                           â”‚  9. Call LLM             â”‚
   â”‚                           â”œâ”€â”€POST /api/generateâ”€â”€â”€â”€â”€>â”‚
   â”‚                           â”‚<â”€â”€â”€â”€responseâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  (waiting, 120s timeout)  â”‚                           â”‚
   â”‚                           â”‚  10. Store assistant msg  â”‚
   â”‚                           â”‚  11. Update memory        â”‚
   â”‚<â”€â”€SendMessageResponseâ”€â”€â”€â”€â”€â”€â”¤                           â”‚
   â”‚    {conversation_id,       â”‚                           â”‚
   â”‚     message_id,            â”‚                           â”‚
   â”‚     answer,                â”‚                           â”‚
   â”‚     metadata}              â”‚                           â”‚
   â”‚                           â”‚                           â”‚
   â”‚  12. Reload messages       â”‚                           â”‚
   â”œâ”€GET /api/chat/ui/conv/{id}>                           â”‚
   â”‚<â”€â”€{conversation, msgs}â”€â”€â”€â”€â”€â”¤                           â”‚
   â”‚                           â”‚                           â”‚
   â”‚  13. Render messages       â”‚                           â”‚
   â”‚  14. Show response         â”‚                           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 9. CONFIGURATION FLOW: LLM SETUP

```
User Interface              Backend API                 External LLM
   â”‚                           â”‚                           â”‚
   â”‚ 1. Open Settings          â”‚                           â”‚
   â”‚ 2. Go to LLM Config       â”‚                           â”‚
   â”‚                           â”‚                           â”‚
   â”‚ 3. Load current config    â”‚                           â”‚
   â”œâ”€â”€â”€GET /api/llm/configâ”€â”€â”€â”€>â”‚                           â”‚
   â”‚<â”€â”€{base_url, model, ...}â”€â”€â”¤                           â”‚
   â”‚                           â”‚                           â”‚
   â”‚ 4. Input new URL          â”‚                           â”‚
   â”‚ 5. Click "Test & Save"    â”‚                           â”‚
   â”‚                           â”‚                           â”‚
   â”œâ”€PUT /api/llm/configâ”€â”€â”€â”€â”€â”€â”€>                           â”‚
   â”‚  {base_url: "http://...", â”‚  Normalize URL            â”‚
   â”‚   default_model: "..."}   â”‚                           â”‚
   â”‚                           â”‚  6. Test connectivity     â”‚
   â”‚                           â”œâ”€â”€GET /api/tagsâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
   â”‚                           â”‚<â”€â”€â”€â”€modelsâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚                           â”‚                           â”‚
   â”‚                           â”‚  7. (if /api/tags fails)  â”‚
   â”‚                           â”‚     Try /v1/models        â”‚
   â”‚                           â”œâ”€â”€GET /v1/modelsâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
   â”‚                           â”‚<â”€â”€â”€â”€modelsâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚                           â”‚                           â”‚
   â”‚                           â”‚  8. (if both fail)        â”‚
   â”‚                           â”‚     Try fallback ports    â”‚
   â”‚                           â”‚     (9000,4000,8000,3000) â”‚
   â”‚                           â”‚                           â”‚
   â”‚                           â”‚  9. Save to .env          â”‚
   â”‚                           â”‚  10. Clear cache          â”‚
   â”‚                           â”‚                           â”‚
   â”‚<â”€â”€{success, config}â”€â”€â”€â”€â”€â”€â”€â”€â”¤                           â”‚
   â”‚                           â”‚                           â”‚
   â”‚ 11. Redirect to Chat      â”‚                           â”‚
   â”‚ 12. Models now available  â”‚                           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. SUMMARY TABLE: KEY ENDPOINTS

| Endpoint | Method | Frontend Call | Backend Logic | LLM Interaction |
|----------|--------|---------------|---------------|-----------------|
| `/api/chat/ui/models` | GET | `loadModels()` | `probe_models()` | Calls `/api/tags` or `/v1/models` |
| `/api/chat/ui/profiles` | GET | `loadRoutingProfiles()` | Query database or return fallback | None |
| `/api/chat/ui/conversations` | GET | `loadConversations()` | Query all conversations | None |
| `/api/chat/ui/conversations` | POST | `handleNewConversation()` | Create Conversation record | None |
| `/api/chat/ui/conversations/{id}` | GET | `loadConversationMessages()` | Query messages for conversation | None |
| `/api/chat/ui/conversations/{id}` | DELETE | `handleDeleteConversation()` | Soft delete conversation | None |
| `/api/chat/ui/send` | POST | `handleSendMessage()` | Create messages, route, call LLM | Calls LLM for response |
| `/api/llm/config` | GET | Settings page | Return current config from .env | None |
| `/api/llm/config` | PUT | Settings page | Validate, probe, save to .env | Probes connectivity |
| `/api/llm/test` | POST | Settings page | Call LLM with test prompt | Calls LLM |
| `/v1/chat` | POST | Internal only | Call OrchestrationGraph | Calls LLM via graph |

---

## 11. DEPLOYMENT CHECKLIST

- [ ] Backend `.env` has correct `LLM_BASE_URL`
- [ ] Backend `.env` has `LLM_TIMEOUT_SECONDS=120` (or higher for slow servers)
- [ ] Frontend `ChatStudio.tsx` has matching 120s timeout in axios calls
- [ ] Database is initialized (SQLite or PostgreSQL)
- [ ] LLM server is running and accessible at configured URL
- [ ] Port 8000 (backend) and 3000 (frontend) are not in use
- [ ] CORS is configured if frontend and backend on different origins
- [ ] All error responses include helpful messages
- [ ] Soft delete logic properly filters deleted records in list endpoints

---

## 12. RECOMMENDATIONS FOR IMPROVEMENT

### HIGH PRIORITY
1. **Implement request/response timeout sync**: Store timeout in .env, read from frontend
2. **Add streaming responses**: Improve UX for long-running operations
3. **Implement message streaming**: Show tokens as they generate
4. **Add error detail API**: Return structured error codes, not just messages
5. **Add memory inspection endpoint**: Let users see/clear conversation memory

### MEDIUM PRIORITY
6. **Implement transaction safety**: Wrap multi-step operations in database transactions
7. **Add routing profile validation**: Backend should validate unknown profiles
8. **Implement soft delete filtering**: Ensure list endpoints properly filter deleted records
9. **Add tool execution debugging**: Return tool inputs/outputs, not just names
10. **Add conversation export**: Let users download conversation history

### LOW PRIORITY
11. **Add API versioning**: Update non-versioned endpoints to use `/v1/`
12. **Add rate limiting**: Prevent abuse on configuration and chat endpoints
13. **Add request logging**: Log all API calls for debugging
14. **Add performance monitoring**: Track response times, token usage, costs

